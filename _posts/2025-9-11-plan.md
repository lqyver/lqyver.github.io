---
title: Plan ? :)
categories: [Plan,First Project Plan ]
tags: [计划]     # TAG names should always be lowercase
---
# 关于开发一个将自动获取网络上的信并使用LLM分析的脚本与量化交易相结合的程序的计划 
 或者叫设想差不多（）
 太难了对我这种半吊子来说😭
 是块硬骨头，慢慢啃吧，没个半年出不来一点结果的估计🥹

## 爬虫脚本部分
--------------------------------------------------
一、整体流水线鸟瞰
--------------------------------------------------
```
URL 列表 → 异步下载 → HTML 缓存 → 多策略内容提取 → 摘要 → 去重 → 汇总 → 输出 JSON/HTML
```

整条链子是**纯异步**的，瓶颈在网络 IO 和模型推理，CPU 友好小模型 + 缓存之后，普通笔记本 10 条新闻 30 秒左右能跑完。

--------------------------------------------------
二、配置中心（Config）
--------------------------------------------------
集中放所有可调参数，方便以后做「环境变量 + 配置文件」扩展。

关键字段解释：
- `MAX_CONCURRENT`：同时并发的 TCP 连接上限，站点一多就往下调（防封）。  
- `BASE_DELAY / RETRY_BACKOFF`：失败重试指数退避，第一次 0.5 s，第二次 1 s，第三次 2 s…  
- `HF_MODEL`：这次用的 CPU 小模型，只有 350 MB，推理速度是 bart-large-cnn 的 3 倍。  
- `DUPLICATE_THRESHOLD=0.75`：两篇内容相似度 > 75 % 就丢掉后者，可调。

--------------------------------------------------
三、日志 & 进度条
--------------------------------------------------
logging 标准库输出到控制台；tqdm 只是装饰器，没装就自动退化到普通 asyncio.gather，不影响功能。

--------------------------------------------------
四、缓存（aiocache）
--------------------------------------------------
默认用的**内存缓存**（SimpleMemoryCache），TTL 24 h。  
重启进程就清空，若要做持久化，换成 RedisCache 或自己落盘 JSON 皆可，接口一致。

--------------------------------------------------
五、数据结构（ExtractedContent）
--------------------------------------------------
一个 `@dataclass` 把后面所有字段锁死，**类型提示 + 自动生成构造函数**，省得写一堆 `self.xxx = xxx`。

注意 `keywords: List[str] = field(default_factory=list)`  
——这样实例化时如果不给 keywords，就会默认空列表，避免之前「缺失参数」的报错。

--------------------------------------------------
六、智能调度器（IntelligentScheduler）
--------------------------------------------------
每个域名维护 3 个指标：
- ok：成功次数  
- fail：失败次数  
- avg_time：最近平均响应时间  

`delay()` 返回**动态延迟**：  
```
延迟 = avg_time * (1 + fail_rate * 3)
```
fail 越多就越慢，上限 5 s；成功越多就越快，下限 0.3 s。  
实现了一个**最简易的自适应限速**，不用手写黑名单。

--------------------------------------------------
七、内容去重（ContentDeduplicator）
--------------------------------------------------
1. 把文本拆成 `Counter({"apple":2, "banana":1})` 词袋。  
2. 两篇内容算 Jaccard 相似度 = 交集 / 并集。  
3. 遍历列表时保留**第一个**相似度 < 0.75 的，其余丢弃。  
复杂度 O(n²)，百篇以内毫无压力；若上十万篇，可改用 MinHash / 向量聚类。

--------------------------------------------------
八、多策略内容提取（SmartExtractor）
--------------------------------------------------
同一个 HTML 依次用 3 个工具抽，谁先抽出非空文字就用谁：

| 工具        | 优点                     | 缺点                 |
|-------------|--------------------------|----------------------|
| trafilatura | 精度高、自动输出日期作者 | 部分站会提取空       |
| newspaper3k | 元信息丰富、关键词       | 对中文站偶尔解析失败 |
| BeautifulSoup4 | 万金油、保底           | 噪音多、无日期       |

返回统一格式：`dict(content=..., title=..., author=..., publish_date=..., keywords=..., extraction_method=...)`  
只要 `content` 长度 > 30 词才算有效，否则继续尝试下一个工具。

--------------------------------------------------
九、摘要器（Summarizer）
--------------------------------------------------
**两级回退**：
1. 能用 transformers 就用小模型 distilbart-cnn-6-6，显存占用 < 500 MB，CPU 也能跑。  
2. 模型加载失败 or 文本太短，就用「句号截断」简单摘要。

`__call__` 接口统一，外部无感知。

--------------------------------------------------
十、异步爬虫（AsyncScraper）
--------------------------------------------------
`async with AsyncScraper() as scraper:` 进入时自动创建 aiohttp 会话，退出时自动关闭连接池。

关键函数：
- `_fetch()`  
  – 用 `Semaphore` 控制并发。  
  – 指数退避重试。  
  – 每次请求前后调 `scheduler.update()` 更新域名统计。  

- `_scrape_one()`  
  – 先读缓存 → 失败再下载 → 保存缓存。  
  – 调用 SmartExtractor 拿到正文 → 调 Summarizer 得到摘要 → 组装 ExtractedContent。  

- `scrape()`  
  – 把一堆 `_scrape_one` 包成任务，用 `tqdm_asyncio.gather` 带进度条等待。

--------------------------------------------------
十一、多源整合（MultiSourceSummarizer）
--------------------------------------------------
把所有文章合并成一篇「大文本」，再跑一遍摘要器，得到**全局摘要**；  
关键词简单做词频统计取 Top10；  
最终输出一个 dict，方便后续写 JSON、写 HTML、入库、发 MQ。

--------------------------------------------------
十二、HTML 报表（build_html_report）
--------------------------------------------------
用 Jinja2 模板（语法类似 Django）渲染，样式内嵌，**单文件即可发给别人**。  
表头：综合摘要 + 关键词 Tag + 来源列表  
表体：每篇文章的标题（可点）、摘要、作者、发布时间、提取方式

--------------------------------------------------
十三、主入口（AdvancedAggregator）
--------------------------------------------------
`run()` 做 4 件事：  
1. 并发爬 → 2. 去重 → 3. 汇总 → 4. 写 JSON、写 HTML、弹浏览器。

文件名带时间戳，多次运行不覆盖。

--------------------------------------------------
十四、CLI（main）
--------------------------------------------------
默认只爬 2 个示例站，10 秒跑通。  
想聚合更多内容，把 `urls` 换成自己的列表即可；也可改成读取文件、读取搜索引擎 API。

--------------------------------------------------
--------------------------------------------------



## 量化部分
------------------------------------------------
1. 全局环境与三方库
------------------------------------------------
```python
import akshare as ak
...
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False
```
- akshare：免费、开源、实时的 A 股/期货/宏观数据源，接口偶尔变动，建议锁版本号（akshare==1.x.x）。  
- SimHei：Windows 自带，Linux/Mac 常没有，可换成 `plt.rcParams['font.sans-serif'] = ['DejaVu Sans']` 或手动下载 SimHei.ttf。  
- colorlog：彩色日志，调试时一眼区分 INFO / ERROR，生产环境可关掉。  

------------------------------------------------
2. 日志系统
------------------------------------------------
```python
handler = colorlog.StreamHandler()
...
logger.setLevel(logging.INFO)
```
- 用 `logger.info/error(...)` 代替 `print`，后续若接入文件日志、Telegram、钉钉机器人，只需加 Handler 即可。  
- 日志里加了 emoji（📂/✅），Windows Terminal/Powershell 可能乱码，可删掉。  

------------------------------------------------
3. 配置区
------------------------------------------------
```python
CACHE_DIR = '.cache'
WORKERS = 16
MYSQL_URL = os.getenv('MYSQL_URL')
```
- `.cache` 放在项目根目录，Git 建议加进 `.gitignore`。  
- `WORKERS` 仅用在「尚未出现的」`fetch_multi` 函数里（脚本里其实没实现），先占坑。  
- `MYSQL_URL` 格式：  
  `mysql+pymysql://user:password@ip:3306/db?charset=utf8mb4`  
  若只研究单机，把 MYSQL_URL 留空即可，不报错。  

------------------------------------------------
4. 工具函数
------------------------------------------------
4.1 缓存键生成  
```python
def _cache_path(code, start, end, adj):
    key = f'{code}_{start}_{end}_{adj}'
    return os.path.join(CACHE_DIR, hashlib.md5(key.encode()).hexdigest() + f'_{adj}.pkl')
```
- 用 MD5 把「股票+起止+复权」映射成 32 位十六进制，避免文件名过长。  
- 末尾再拼 `_{adj}.pkl`，方便人工肉眼区分复权方式。  

4.2 原始数据拉取  
```python
def fetch_daily(code, start, end, adj='qfq'):
    df = ak.stock_zh_a_hist(...)
    ...
    return df[df['volume']>0]
```
- akshare 返回的列是中文，脚本硬编码 rename，akshare 升级后列名若变，这里会 KeyError，建议加 `assert set(df.columns) >= {'日期','开盘','收盘','最高','最低','成交量'}`。  
- 去掉成交量为 0 的记录（停牌日），防止均线计算出现 NaN。  

4.3 缓存层 + MySQL 层  
```python
def fetch_daily_cached(...)
    ...
    if os.path.exists(path): ...
    ...
    if MYSQL_URL:
        engine = create_engine(MYSQL_URL)
        df.assign(code=code).to_sql('daily', ..., if_exists='append', index=True, method='multi')
```
- 缓存优先：同请求第二次毫秒级返回。  
- 落库：  
  – 表名 `daily`，主键 `(code, date)` 建议后面手动加联合索引，否则随着数据量增大，查询会变慢。  
  – `if_exists='append'` 导致重复运行会插入重复行，可在 MySQL 端加 `UNIQUE KEY(code,date)`，利用 `INSERT IGNORE` 或 `ON DUPLICATE KEY UPDATE` 去重。  
- 未做「增量更新」：每次 `start~end` 区间变化就重新拉全量。后面提供了 `increment` 标志位，但函数 `append_daily` 并未实现，留作作业。  

------------------------------------------------
5. 策略逻辑：strat_dual_ma
------------------------------------------------
```python
def strat_dual_ma(df, short=20, long=60, cost=0.0001, slip=0.0002):
```
- 参数说明  
  – cost：双边佣金+印花税合计约万 1（0.0001）。  
  – slip：滑点万 2（0.0002），模拟开盘无法按收盘价成交的损耗。  
- 信号生成  
  – `pos = 1` 表示满仓，`pos = 0` 表示空仓；shift(1) 把「当日信号」挪到「次日执行」，避免未来函数。  
- 费用计算  
  – 只在换仓日（`trades=1`）收取费用，总费率 `cost + slip = 0.03%`，对高频参数影响极大。  
- 返回字段  
  – `str_ret`：策略日收益；`str_nav`：策略净值；`buy_nav`：买入持有净值。  

------------------------------------------------
6. 绩效评估：evaluate
------------------------------------------------
```python
def evaluate(df):
    ann = df['str_ret'].mean() * 252
    ...
```
- 仅给年化收益，没算波动率、夏普、最大回撤。可自行加：  
```python
def sharpe(returns):
    return returns.mean() / returns.std() * np.sqrt(252)
def max_dd(nav):
    roll_max = nav.cummax()
    drawdown = nav / roll_max - 1
    return drawdown.min()
```

------------------------------------------------
7. 可视化：plot
------------------------------------------------
```python
plt.yscale('log')
```
- 对数坐标方便对比「远离 1」的曲线，但 A 股短期波动小，线性坐标也可。  

------------------------------------------------
8. 网格优化：optimize_params
------------------------------------------------
```python
for s, l in product(short_range, long_range):
    if s >= l: continue
```
- 暴力循环，复杂度 O(n²)。50×15=750 次回测，单标的 10 年数据 0.2s/次，约 2~3 分钟跑完。  
- 提前剪枝 `s >= l`，避免无意义组合。  

------------------------------------------------
9. 热力图：plot_heatmap
------------------------------------------------
- 用 `seaborn.heatmap` 可视化年化收益矩阵，一眼看出「高原区」。  
- 若参数高原平坦，说明策略对参数不敏感，稳健；若尖峰，则实盘中参数易失效。  

------------------------------------------------
10. CLI 入口
------------------------------------------------
```python
@click.group()
def cli(): pass
```
三条命令：  
1. download：批量下载 + 缓存 + MySQL。  
2. backtest：单标的快速回测。  
3. optimize：网格搜索 + 热力图 + 最优参数再跑一次并画图。  

------------------------------------------------
11. 可踩的坑 & 细节补丁
------------------------------------------------
| 问题 | 现象 | 解决 |
|---|---|---|
| akshare 列名变化 | KeyError | 加 assert / try-rename |
| 缓存键未含复权因子 | 切换前/后复权仍读到旧缓存 | 已在文件名带 `adj` |
| MySQL 重复插入 | 主键冲突 | 在 MySQL 建 unique key 或者用 `to_sql(..., method='multi', if_exists='append')` 前先做 `REPLACE INTO` |
| 中文乱码 | Linux 画图标点成方块 | 安装 SimHei 或换 DejaVu Sans |
| 滑点太小 | 高频信号（5/10）年化虚高 | 可把 slip 调到 0.0005~0.001 |
| 未来函数 | 信号未 shift | 脚本已 shift(1)，放心 |

------------------------------------------------
12. 二次开发思路
------------------------------------------------
1. 多标的并行  
```python
with ThreadPoolExecutor(WORKERS) as pool:
    pool.map(lambda c: fetch_daily_cached(c, start, end, adj), codes)
```
2. 贝叶斯优化  
用 `optuna.create_study(direction='maximize')`，把 `strat_dual_ma` 封装成  
```python
def objective(trial):
    s = trial.suggest_int('short', 5, 50, step=5)
    l = trial.suggest_int('long', 50, 200, step=10)
    ...
    return sharpe(tmp['str_ret'])
```
3. 多策略组合  
- 把 dma、布林带、动量统一封装成 `Strategy` 基类，返回 `pos` 序列，再组合 `w1*dma + w2*boll`。  
4. 实盘下单  
- 日内 5 分钟线跑完信号后，调用 `xtquant.xttrader` 或 `easytrader` 直接下单，仓位同步到 MySQL。  
5. Web 可视化  
- 用 Streamlit / Dash 把「下载-回测-优化」做成网页，参数拖拉条实时出图。  

------------------------------------------------




## 结合部分

完全没做................

前两个我都还没搞清楚😢

等我把前面的都弄明白，分别出了两个独立的，功能完全的demo再说结合的事吧

